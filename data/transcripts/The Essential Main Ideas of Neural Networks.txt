 Neural networks seem so complicated but then not. StatQuest. Hello, I'm Josh Starmer and welcome to StatQuest. Today we're going to talk about neural networks part one inside the black box. Neural networks, one of the most popular algorithms in machine learning, cover a broad range of concepts and techniques. However, people call them a black box because it can be hard to understand what they're doing. The goal of this series is to take a peek into the black box by breaking down each concept and technique into its components and walking through how they fit together step by step. In this first part, we will learn about what neural networks do and how they do it. In part two, we'll talk about how neural networks are fit to data with back propagation. Then we will talk about variations on the simple neural network presented in this part, including deep learning. Note. Crazy, awesome news. I have a new way to think about neural networks that will help beginners and seasoned experts alike gain a deep insight into what neural networks do. For example, most tutorials use cool looking but hard to understand graphs and fancy mathematical notation to represent neural networks. In contrast, I'm going to label every little thing on the neural network to make it easy to keep track of the details. In the math, we'll be as simple as possible while still being true to the algorithm. These differences will help you develop a deep understanding of what neural networks actually do. So, with that said, let's imagine we tested a drug that was designed to treat an illness and we gave the drug to three different groups of people with three different dosages. Low, medium, and high. The low dosages were not effective, so we set them to zero on this graph. In contrast, the medium dosages were effective, so we set them to one. And the high dosages were not effective, so those are set to zero. Now that we have this data, we would like to use it to predict whether or not a future dosage will be effective. However, we can't just fit a straight line to the data to make predictions, because no matter how we rotate the straight line, it can only accurately predict two of the three dosages. The good news is that a neural network can fit a squiggle to the data. The green squiggle is close to zero for low dosages, close to one for medium dosages, and close to zero for high dosages. And even if we have a really complicated data set like this, a neural network can fit a squiggle to it. In this stack quest, we're going to use this super simple data set and show how this neural network creates this green squiggle. But first, let's just talk about what a neural network is. A neural network consists of nodes and connections between the nodes. Note, the numbers along each connection represent parameter values that were estimated when this neural network was fit to the data. For now, just know that these parameter estimates are analogous to the slope and intercept values that we saw for when we fit a straight line to data. Likewise, a neural network starts out with unknown parameter values that are estimated when we fit the neural network to a data set using a method called back propagation. And we will talk about how back propagation estimates these parameters in part two in this series. But for now, just assume that we've already fit this neural network to this specific data set. And that means we have already estimated these parameters. Also, you may have noticed that some of the nodes have curved lines inside of them. These bent or curved lines are the building blocks for fitting a squiggle to data. The goal of this stack quest is to show you how these identical curves can be reshaped by the parameter values and then added together to get a green squiggle that fits the data. Note, there are many common bent or curved lines that we can choose for a neural network. This specific curved line is called soft plus, which sounds like a brand of toilet paper. Alternatively, we could use this bent line called RLU, which is short for rectified linear unit and sounds like a robot. Or we could use a sigmoid shape or any other bent or curved line. Oh no, it's the dreaded terminology alert. The curved or bent lines are called activation functions. When you build a neural network, you have to decide which activation function or functions you want to use. In most people teach neural networks, they use the sigmoid activation function. However, in practice, it is much more common to use the RLU activation function, or the soft plus activation function. So we'll use the soft plus activation function in this stack quest. Anyway, we'll talk more about how you choose activation functions later in this series. Note, this specific neural network is about as simple as they get. It only has one input node where we plug in the dosage. Only one output node to tell us the predicted effectiveness, and only two nodes between the input and output nodes. However, in practice, neural networks are usually much fancier. And have more than one input node, more than one output node, different layers of nodes between the input and output nodes, and a spider web of connections between each layer of nodes. Oh no, it's another terminology alert. These layers of nodes between the input and output nodes are called hidden layers. When you build a neural network, one of the first things you do is decide how many hidden layers you want, and how many nodes go into each hidden layer. Although there are rules of thumb for making decisions about the hidden layers, you essentially make a guess and see how well the neural network performs, adding more layers and nodes if needed. Now, even though this neural network looks fancy, it is still made from the same parts, used in this simple neural network, which has only one hidden layer with two nodes. So let's learn how this neural network creates new shapes from the curved or bent lines in the hidden layer, and then add them together to get a green squiggle that fits the data. Note, to keep the math simple, let's assume dosages go from zero for low to one for high. The first thing we are going to do is plug the lowest dosage zero into the neural network. Now, to get from the input node to the top node in the hidden layer, this connection multiplies the dosage by negative 34.4, and then adds 2.14. And the result is an x-axis coordinate for the activation function. For example, the lowest dosage zero is multiplied by negative 34.4, and then we add 2.14 to get 2.14 as the x-axis coordinate for the activation function. To get the corresponding y-axis value, we plug 2.14 into the activation function, which, in this case, is the soft plus function. Note, if we had chosen the sigmoid curve for the activation function, then we would plug 2.14 into the equation for the sigmoid curve. And if we had chosen the real u bent line for the activation function, then we would plug 2.14 into the real u equation. But since we are using soft plus for the activation function, we plug 2.14 into the soft plus equation. And the log of 1 plus e raised to the 2.14 power is 2.25. Note, in statistics, machine learning, and most programming languages, the log function implies the natural log or the log base e. Anyway, the y-axis coordinate for the activation function is 2.25. So let's extend this y-axis up a little bit, and put a blue dot at 2.25 for when dosage equal zero. Now, if we increase the dosage a little bit and plug 0.1 into the input, the x-axis coordinate for the activation function is negative 1.3. And the corresponding y-axis value is 0.24. So let's put a blue dot at 0.24 for when dosage equal 0.1. And if we continue to increase the dosage values all the way to 1, the maximum dosage, we get this blue curve. Note, before we move on, I want to point out that the full range of dosage values from 0 to 1 corresponds to this relatively narrow range of values from the activation function. In other words, when we plug dosage values from 0 to 1 into the neural network, and then multiply them by negative 34.4 and add 2.14, we only get x-axis coordinates that are within the red box. And thus, only the corresponding y-axis values in the red box are used to make this new blue curve. Bam! Now we scale the y-axis values for the blue curve by negative 1.3. For example, when dosage equals 0, the current y-axis coordinate for the blue curve is 2.25. So we multiply 2.25 by negative 1.3 and get negative 2.93. And negative 2.93 corresponds to this position on the y-axis. Likewise, we multiply all of the other y-axis coordinates on the blue curve by negative 1.3. And we end up with a new blue curve. Bam! Now let's focus on the connection from the input node to the bottom node in the hidden layer. However, this time we multiply the dosage by negative 2.52, instead of negative 34.4. And we add 1.29 instead of 2.14 to get the x-axis coordinate for the activation function. Remember, these values come from fitting the neural network to the data with back propagation, and we'll talk about that in part 2 in this series. Now, if we plug the lowest dosage 0 into the neural network, then the x-axis coordinate for the activation function is 1.29. Now we plug 1.29 into the activation function to get the corresponding y-axis value. And get 1.53. And that corresponds to this yellow dot. Now we just plug in dosage values from 0 to 1 to get the corresponding y-axis values. And we get this orange curve. Note, just like before, I want to point out that the full range of dosage values from 0 to 1, corresponds to this narrow range of values from the activation function. In other words, when we plug dosage values from 0 to 1 into the neural network, we only get x-axis coordinates that are within the red box. And thus, only the corresponding y-axis values in the red box are used to make this new orange curve. So we see that fitting a neural network to data gives us different parameter estimates on the connections, and that results in each node in the hidden layer using different portions of the activation functions to create these new and exciting shapes. Now, just like before, we scale the y-axis coordinates on the orange curve. Only this time, we scale by a positive number 2.28. And that gives us this new orange curve. Now the neural network tells us to add the y-axis coordinates from the blue curve to the orange curve. And that gives us this green squiggle. Then, finally, we subtract 0.58 from the y-axis values on the green squiggle, and we have a green squiggle that fits the data. Bam! Now, if someone comes along and says that they are using dosage equal to 0.5, we can look at the corresponding y-axis coordinate on the green squiggle and see that the dosage will be effective. Or, we can solve for the y-axis coordinate by plugging dosage equal 0.5 into the neural network, and do the math. rhyms to out nice. So, turn this over! Then we start from theρο field right now. What is this sound like because we need my ear? And we see that the y-axis coordinate on the green squiggle is 1.03. And since 1.03 is closer to 1, then 0, we will conclude that a dosage equal to 0.5 is effective. Double bound. Now, if you've made it this far, you may be wondering why this is called a neural network. Instead of a big fancy squiggle fitting machine. The reason is that way back in the 1940s and 50s when neural networks were invented, they thought the nodes were vaguely like neurons. And the connections between the nodes were sort of like synapses. However, I think they should be called big fancy squiggle fitting machines, because that's what they do. Note, whether or not you call it a squiggle fitting machine, the parameters that we multiply are called weights, and the parameters that we add are called biases. Note, this neural network starts with two identical activation functions, but the weights and biases on the connections slice them, flip them, and stretch them into new shapes. Which are then added together to get a squiggle that is entirely new. And then the squiggle is shifted to fit the data. Now, if we can create this green squiggle with just two nodes in a single hidden layer, just imagine what types of green squiggles we could fit with more hidden layers, and more nodes in each hidden layer. In theory, neural networks can fit a green squiggle to just about any data set, no matter how complicated. And I think that's pretty cool. Triple BAM! Now it's time for some, shameless self-promotion. If you want to review statistics and machine learning offline, check out the stat quest study guides at statquest.org. There's something for everyone. Hooray! We've made it to the end of another exciting stat quest. If you like this stat quest and want to see more, please subscribe. And if you want to support stat quest, consider contributing to my Patreon campaign. Becoming a channel member, buying one or two of my original songs or a t-shirt or a hoodie, or just donate, the links are in the description below. All right, until next time, quest on!